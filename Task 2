1. Spark Setup

Code :
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("DataProcessingExample").getOrCreate()

# Verify Spark session
print("Spark Session Created Successfully!")

Output:

Spark Session Created Successfully!

2. Data Ingestion

Code :

# Load CSV data
df = spark.read.csv("path/to/data.csv", header=True, inferSchema=True)

# Show the data
df.show()

Output :

+---+-------+----------+------+
| id|   name|department|salary|
+---+-------+----------+------+
|  1|   John|    Sales | 50000|
|  2|   Jane|Marketing | 60000|
|  3|Michael|  IT      | 75000|
|  4|  Sarah|   HR     | 55000|
+---+-------+----------+------+

3. Data Processing

Code :

# Filter data (e.g., employees with salary > 60000)
filtered_df = df.filter(df["salary"] > 60000)
filtered_df.show()

# Aggregate data (e.g., average salary by department)
aggregated_df = df.groupBy("department").agg({"salary": "avg"})
aggregated_df.show()

Output :

+---+-------+----------+------+
| id|   name|department|salary|
+---+-------+----------+------+
|  3|Michael|        IT| 75000|
+---+-------+----------+------+

4. Data Analysis

Code :

# Find the top N highest salaries (e.g., top 2)
top_n_salaries = df.orderBy(df["salary"].desc()).limit(2)
top_n_salaries.show()

Output :
+---+-------+----------+------+
| id|   name|department|salary|
+---+-------+----------+------+
|  3|Michael|        IT| 75000|
|  2|   Jane|Marketing | 60000|
+---+-------+----------+------+

5. Save Processed Data

Output :

# Save filtered data as CSV
filtered_df.write.csv("path/to/output/filtered_data.csv", header=True)

# Save aggregated data as JSON
aggregated_df.write.json("path/to/output/aggregated_data.json")

Output:

filtered_data.csv containing the filtered data.

aggregated_data.json containing the aggregated data.
